# -*- coding: utf-8 -*-
"""restoration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fHqKSKpFOE4zyq_3bCfLr8MJuAzwBRUw
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/data/photos
!mkdir -p /content/data/buildings

!unzip -q /content/drive/MyDrive/heritage_restoration_data/photos_dataset.zip     -d /content/data/photos/
!unzip -q /content/drive/MyDrive/heritage_restoration_data/buildings_dataset.zip   -d /content/data/buildings/

!ls /content/data/photos
!ls /content/data/buildings

"""## Imports + Path Setup"""

!pip install -q diffusers transformers accelerate safetensors controlnet-aux opencv-python pillow tqdm

import os
import glob
from PIL import Image
import torch
from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using:", device)

PHOTOS_RAW      = "/content/data/photos/raw"
PHOTOS_EDGES    = "/content/data/photos/edges"

BUILDINGS_RAW   = "/content/data/buildings/raw"
BUILDINGS_EDGES = "/content/data/buildings/edges"

OUT_PHOTOS      = "/content/results_stage2/photos"
OUT_BUILDINGS   = "/content/results_stage2/buildings"

os.makedirs(OUT_PHOTOS, exist_ok=True)
os.makedirs(OUT_BUILDINGS, exist_ok=True)

"""##Load Stable Diffusion + ControlNet"""

from diffusers import ControlNetModel, StableDiffusionControlNetImg2ImgPipeline, UniPCMultistepScheduler

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
).to(device)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

print("Pipeline ready.")

"""## Restoration function"""

def restore_batch(
    raw_dir, edge_dir, out_dir,
    strength, guidance, steps,
    prompt, negative
):
    os.makedirs(out_dir, exist_ok=True)

    images = sorted(glob.glob(f"{raw_dir}/*"))
    edges = {os.path.basename(e): e for e in glob.glob(f"{edge_dir}/*")}

    for img_path in tqdm(images):
        fname = os.path.basename(img_path)

        edge_name = (
            fname.replace(".jpg", "_edges.png")
                 .replace(".jpeg", "_edges.png")
        )

        if edge_name not in edges:
            print("Missing edge for:", fname, "| expected:", edge_name)
            continue

        base = Image.open(img_path).convert("RGB").resize((512,512))
        cond = Image.open(edges[edge_name]).convert("RGB").resize((512,512))

        out = pipe(
            prompt=prompt,
            negative_prompt=negative,
            image=base,
            control_image=cond,
            strength=strength,
            guidance_scale=guidance,
            num_inference_steps=steps
        ).images[0]

        out.save(os.path.join(out_dir, fname))

"""## Processing the dataset"""

restore_batch(
    PHOTOS_RAW, PHOTOS_EDGES, OUT_PHOTOS,
    strength=0.30,
    guidance=7.0,
    steps=20,
    prompt="high-quality restoration, realistic, sharp, detailed, clean",
    negative="blur, extra edges, distortions, artifacts, deformed structure"
)

!rm -rf /content/data/buildings/raw_small
!rm -rf /content/data/buildings/edges_small

!mkdir /content/data/buildings/raw_small
!mkdir /content/data/buildings/edges_small

import glob, os, shutil, random

RAW_DIR = "/content/data/buildings/raw"
EDGE_DIR = "/content/data/buildings/edges"

raw_files = sorted(glob.glob(f"{RAW_DIR}/*.jpg"))
edge_files = sorted(glob.glob(f"{EDGE_DIR}/*.png"))

edge_names = [os.path.basename(e) for e in edge_files]

matched_pairs = []

for raw in raw_files:
    fname = os.path.basename(raw)
    edge_name = fname.replace(".jpg", "_edges.png")
    if edge_name in edge_names:
        matched_pairs.append((raw, os.path.join(EDGE_DIR, edge_name)))

print("Total matched building images:", len(matched_pairs))

chosen_30 = random.sample(matched_pairs, 30)

RAW_SMALL = "/content/data/buildings/raw_small"
EDGE_SMALL = "/content/data/buildings/edges_small"

for raw_path, edge_path in chosen_30:
    shutil.copy(raw_path, RAW_SMALL)
    shutil.copy(edge_path, EDGE_SMALL)

print("Copied 30 matched images.")

print("Raw small:", len(os.listdir(RAW_SMALL)))
print("Edge small:", len(os.listdir(EDGE_SMALL)))

restore_batch(
    RAW_SMALL,
    EDGE_SMALL,
    OUT_BUILDINGS,
    strength=0.35,
    guidance=6.5,
    steps=20,
    prompt="building restoration, realistic texture, clean sharp edges, detailed structure",
    negative="distorted lines, warped architecture, melted surfaces"
)

!zip -qr /content/results_stage2.zip /content/results_stage2
!cp /content/results_stage2.zip /content/drive/MyDrive/heritage_restoration_data/

"""##comparison images (Before vs After grid)"""

from PIL import Image
import matplotlib.pyplot as plt
import os

RAW = "/content/data/photos/raw"
REST = "/content/results_stage2/photos"

samples = random.sample(sorted(os.listdir(RAW)), 5)

plt.figure(figsize=(14, 18))

for i, fname in enumerate(samples):
    raw_img = Image.open(os.path.join(RAW, fname)).resize((380, 380))
    rest_img = Image.open(os.path.join(REST, fname)).resize((380, 380))

    plt.subplot(len(samples), 2, 2*i + 1)
    plt.imshow(raw_img)
    plt.title("Raw", fontsize=14)
    plt.axis('off')

    plt.subplot(len(samples), 2, 2*i + 2)
    plt.imshow(rest_img)
    plt.title("Restored", fontsize=14)
    plt.axis('off')

plt.tight_layout()
plt.show()

from PIL import Image
import matplotlib.pyplot as plt
import os

RAW = "/content/data/buildings/raw_small"
REST = "/content/results_stage2/buildings"

samples = random.sample(sorted(os.listdir(RAW)), 5)

plt.figure(figsize=(14, 18))

for i, fname in enumerate(samples):
    raw_img = Image.open(os.path.join(RAW, fname)).resize((380, 380))
    rest_img = Image.open(os.path.join(REST, fname)).resize((380, 380))

    plt.subplot(len(samples), 2, 2*i + 1)
    plt.imshow(raw_img)
    plt.title("Raw", fontsize=14)
    plt.axis('off')

    plt.subplot(len(samples), 2, 2*i + 2)
    plt.imshow(rest_img)
    plt.title("Restored", fontsize=14)
    plt.axis('off')

plt.tight_layout()
plt.show()

"""## Real-Time Restoration"""

!pip install gradio

import gradio as gr
import cv2
import numpy as np
from PIL import Image

def generate_edges(img):
    gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    return Image.fromarray(edges).convert("RGB")


def restore_img(img, prompt, negative, strength, guidance, steps):
    if img is None:
        return None, None

    img512 = img.convert("RGB").resize((512, 512))
    edges = generate_edges(img512)

    result = pipe(
        prompt=prompt,
        negative_prompt=negative,
        image=img512,
        control_image=edges,
        strength=strength,
        guidance_scale=guidance,
        num_inference_steps=steps
    ).images[0]

    return img512, result

def photo_mode(img, strength, steps):
    prompt = "old photo restoration, clean realistic skin, fix damage, restore clarity"
    negative = "distortion, artifacts, warped texture"

    return restore_img(img, prompt, negative, strength, 7.0, steps)

def building_mode(img, strength, steps):
    prompt = "heritage building restoration, sharp edges, clean texture, structural realism"
    negative = "warped lines, melted architecture, distortion"

    return restore_img(img, prompt, negative, strength, 6.5, steps)


with gr.Blocks(title="Heritage Restoration AI") as demo:

    gr.Markdown("""
    # üèõÔ∏è Heritage Restoration AI
    Upload an image to restore it using **Diffusion + ControlNet**
    **Two modes available:**
    - üì∏ Photo Restoration
    - üèõÔ∏è Building Restoration
    """)

    with gr.Tabs():

        with gr.Tab("üì∏ Photo Restoration"):
            inp = gr.Image(type="pil", label="Upload Old Photo")

            strength = gr.Slider(0.1, 1.0, value=0.35, label="Restoration Strength")
            steps = gr.Slider(10, 40, value=20, step=1, label="Diffusion Steps")

            with gr.Row():
                out_raw = gr.Image(label="Original (512x512)")
                out_rest = gr.Image(label="Restored Output")

            btn = gr.Button("Restore Photo")

            btn.click(photo_mode,
                      inputs=[inp, strength, steps],
                      outputs=[out_raw, out_rest])


        with gr.Tab("üèõÔ∏è Building Restoration"):
            inp2 = gr.Image(type="pil", label="Upload Building Image")

            strength2 = gr.Slider(0.1, 1.0, value=0.35, label="Restoration Strength")
            steps2 = gr.Slider(10, 40, value=20, step=1, label="Diffusion Steps")

            with gr.Row():
                out_raw2 = gr.Image(label="Original (512x512)")
                out_rest2 = gr.Image(label="Restored Output")

            btn2 = gr.Button("Restore Building")

            btn2.click(building_mode,
                       inputs=[inp2, strength2, steps2],
                       outputs=[out_raw2, out_rest2])

demo.launch(debug=True)

